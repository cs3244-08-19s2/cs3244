{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import os\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%aimport utils\n",
    "%aimport imc\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdiv = \"/\"\n",
    "### LABELS ###\n",
    "\n",
    "normal_label = 0\n",
    "pneumonia_label = 1\n",
    "labels = sorted([normal_label, pneumonia_label])\n",
    "\n",
    "### IMAGE SETTINGS ###\n",
    "\n",
    "dimension = (64,0)\n",
    "resize_dim = dimension[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_train_normal = \".{}chest_xray{}train{}NORMAL{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "path_train_pneumonia = \".{}chest_xray{}train{}PNEUMONIA{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "\n",
    "path_test_normal = \".{}chest_xray{}test{}NORMAL{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "path_test_pneumonia = \".{}chest_xray{}test{}PNEUMONIA{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "\n",
    "print(\"File name loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Sift Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab2.pkl saved\n",
      "Built 200-word vocabulary from training dataset\n"
     ]
    }
   ],
   "source": [
    "from imc import build_vocabulary_from_dirs\n",
    "import os.path as osp\n",
    "import pickle\n",
    "\n",
    "vocab_filename = 'vocab2.pkl'\n",
    "vocab_size = 200\n",
    "vocab = build_vocabulary_from_dirs(path_train_normal, path_train_pneumonia, vocab_size)\n",
    "with open(vocab_filename, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    \n",
    "print(f\"Built {vocab_size}-word vocabulary from training dataset, saved to {vocab_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image histograms generated for training and test images\n"
     ]
    }
   ],
   "source": [
    "from imc import bags_of_sifts_from_dir\n",
    "\n",
    "data_train_normal = bags_of_sifts_from_dir(path_train_normal, vocab_filename)\n",
    "data_train_pneumonia = bags_of_sifts_from_dir(path_train_pneumonia, vocab_filename)\n",
    "\n",
    "data_test_normal = bags_of_sifts_from_dir(path_test_normal, vocab_filename)\n",
    "data_test_pneumonia = bags_of_sifts_from_dir(path_test_pneumonia, vocab_filename)\n",
    "\n",
    "print(\"Image histograms generated for training and test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data concatenated\n",
      "Train data dimensions: (100, 200)\n",
      "Test data dimensions: (100, 200)\n"
     ]
    }
   ],
   "source": [
    "# Number of training and test images for normal and pneumonia\n",
    "label_train_normal = [0] * 50\n",
    "label_train_pneumonia = [1] * 50\n",
    "label_test_normal = [0] * 50\n",
    "label_test_pneumonia = [1] * 50\n",
    "\n",
    "# Combine training images and labels\n",
    "data_train = np.concatenate((data_train_normal, data_train_pneumonia), axis=0)\n",
    "label_train = np.asarray(label_train_normal + label_train_pneumonia)\n",
    "\n",
    "# Combine training images and labels\n",
    "data_test = np.concatenate((data_test_normal, data_test_pneumonia), axis=0)\n",
    "label_test = np.asarray(label_test_normal + label_test_pneumonia)\n",
    "\n",
    "print(\"Data concatenated\")\n",
    "print(f\"Train data dimensions: {data_train.shape}\")\n",
    "print(f\"Test data dimensions: {data_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets shuffled\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "rand_order = np.random.permutation(data_train.shape[0])\n",
    "data_train = data_train[rand_order]\n",
    "label_train = label_train[rand_order]\n",
    "\n",
    "# # Shuffle\n",
    "rand_order = np.random.permutation(data_test.shape[0])\n",
    "data_test = data_test[rand_order]\n",
    "label_test = label_test[rand_order]\n",
    "\n",
    "print(\"Datasets shuffled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Scale Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15 35]\n",
      " [ 9 41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.30      0.41        50\n",
      "           1       0.54      0.82      0.65        50\n",
      "\n",
      "    accuracy                           0.56       100\n",
      "   macro avg       0.58      0.56      0.53       100\n",
      "weighted avg       0.58      0.56      0.53       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(gamma='scale')\n",
    "svclassifier.fit(data_train, label_train)\n",
    "      \n",
    "# Make prediction\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 30]\n",
      " [ 1 49]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.40      0.56        50\n",
      "           1       0.62      0.98      0.76        50\n",
      "\n",
      "    accuracy                           0.69       100\n",
      "   macro avg       0.79      0.69      0.66       100\n",
      "weighted avg       0.79      0.69      0.66       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='poly', degree=8)\n",
    "svclassifier.fit(data_train, label_train)\n",
    "\n",
    "# Make prediction\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15 35]\n",
      " [ 9 41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.30      0.41        50\n",
      "           1       0.54      0.82      0.65        50\n",
      "\n",
      "    accuracy                           0.56       100\n",
      "   macro avg       0.58      0.56      0.53       100\n",
      "weighted avg       0.58      0.56      0.53       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(data_train, label_train)\n",
    "\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\HuiYing\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25  2]\n",
      " [ 6 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.86        27\n",
      "           1       0.91      0.78      0.84        27\n",
      "\n",
      "    accuracy                           0.85        54\n",
      "   macro avg       0.86      0.85      0.85        54\n",
      "weighted avg       0.86      0.85      0.85        54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='sigmoid')\n",
    "svclassifier.fit(data_train, label_train)\n",
    "\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:svm]",
   "language": "python",
   "name": "conda-env-svm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
