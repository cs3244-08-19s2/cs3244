{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import os\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%aimport utils\n",
    "%aimport imc\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdiv = \"/\"\n",
    "### LABELS ###\n",
    "\n",
    "normal_label = 0\n",
    "pneumonia_label = 1\n",
    "labels = sorted([normal_label, pneumonia_label])\n",
    "\n",
    "### IMAGE SETTINGS ###\n",
    "\n",
    "dimension = (64,0)\n",
    "resize_dim = dimension[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_train_normal = \".{}chest_xray{}train{}NORMAL{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "path_train_pneumonia = \".{}chest_xray{}train{}PNEUMONIA{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "\n",
    "path_test_normal = \".{}chest_xray{}test{}NORMAL{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "path_test_pneumonia = \".{}chest_xray{}test{}PNEUMONIA{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "\n",
    "# number of images to be loaded from each directory\n",
    "train_images_limit = 200\n",
    "test_images_limit = 200\n",
    "load_all_images = False\n",
    "\n",
    "paths_train_normal = utils.extract_image_paths(path_train_normal)\n",
    "paths_train_pneumonia = utils.extract_image_paths(path_train_pneumonia)\n",
    "paths_test_normal = utils.extract_image_paths(path_test_normal)\n",
    "paths_test_pneumonia = utils.extract_image_paths(path_test_pneumonia)\n",
    "\n",
    "if not load_all_images:\n",
    "    paths_train_normal = paths_train_normal[1:train_images_limit]\n",
    "    paths_train_pneumonia = paths_train_pneumonia[1:train_images_limit]\n",
    "    paths_test_normal = paths_test_normal[1:test_images_limit]\n",
    "    paths_test_pneumonia = paths_train_pneumonia[1:test_images_limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Sift Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 200-word vocabulary from training dataset, saved to vocab2.pkl\n"
     ]
    }
   ],
   "source": [
    "# from imc import build_vocabulary_from_dirs\n",
    "import os.path as osp\n",
    "import pickle\n",
    "\n",
    "vocab_filename = 'vocab2.pkl'\n",
    "vocab_size = 200\n",
    "vocab = imc.build_vocabulary_from_dirs(paths_train_normal, paths_train_pneumonia, vocab_size)\n",
    "with open(vocab_filename, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    \n",
    "print(f\"Built {vocab_size}-word vocabulary from training dataset, saved to {vocab_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Image Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image histograms generated for training and test images\n"
     ]
    }
   ],
   "source": [
    "from imc import bags_of_sifts\n",
    "\n",
    "data_train_normal = bags_of_sifts(paths_train_normal, vocab_filename)\n",
    "data_train_pneumonia = bags_of_sifts(paths_train_pneumonia, vocab_filename)\n",
    "\n",
    "data_test_normal = bags_of_sifts(paths_test_normal, vocab_filename)\n",
    "data_test_pneumonia = bags_of_sifts(paths_test_pneumonia, vocab_filename)\n",
    "\n",
    "print(\"Image histograms generated for training and test images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data concatenated\n",
      "Train data dimensions: (5216, 200)\n",
      "Test data dimensions: (624, 200)\n"
     ]
    }
   ],
   "source": [
    "# Number of training and test images for normal and pneumonia\n",
    "\n",
    "label_train_normal = [0] * len(paths_train_normal)\n",
    "label_train_pneumonia = [1] * len(paths_train_pneumonia)\n",
    "label_test_normal = [0] * len(paths_test_normal)\n",
    "label_test_pneumonia = [1] * len(paths_test_pneumonia)\n",
    "\n",
    "# Combine training images and labels\n",
    "data_train = np.concatenate((data_train_normal, data_train_pneumonia), axis=0)\n",
    "label_train = np.asarray(label_train_normal + label_train_pneumonia)\n",
    "\n",
    "# Combine training images and labels\n",
    "data_test = np.concatenate((data_test_normal, data_test_pneumonia), axis=0)\n",
    "label_test = np.asarray(label_test_normal + label_test_pneumonia)\n",
    "\n",
    "print(\"Data concatenated\")\n",
    "print(f\"Train data dimensions: {data_train.shape}\")\n",
    "print(f\"Test data dimensions: {data_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets shuffled\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "rand_order = np.random.permutation(data_train.shape[0])\n",
    "\n",
    "data_train = data_train[rand_order]\n",
    "label_train = label_train[rand_order]\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "rand_order = np.random.permutation(data_test.shape[0])\n",
    "data_test = data_test[rand_order]\n",
    "label_test = label_test[rand_order]\n",
    "\n",
    "print(\"Datasets shuffled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Scale Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5 229]\n",
      " [  0 390]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.04       234\n",
      "           1       0.63      1.00      0.77       390\n",
      "\n",
      "    accuracy                           0.63       624\n",
      "   macro avg       0.82      0.51      0.41       624\n",
      "weighted avg       0.77      0.63      0.50       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(gamma='scale')\n",
    "svclassifier.fit(data_train, label_train)\n",
    "      \n",
    "# Make prediction\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9 225]\n",
      " [  0 390]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.04      0.07       234\n",
      "           1       0.63      1.00      0.78       390\n",
      "\n",
      "    accuracy                           0.64       624\n",
      "   macro avg       0.82      0.52      0.43       624\n",
      "weighted avg       0.77      0.64      0.51       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='poly', degree=8)\n",
    "svclassifier.fit(data_train, label_train)\n",
    "\n",
    "# Make prediction\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5 229]\n",
      " [  0 390]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.04       234\n",
      "           1       0.63      1.00      0.77       390\n",
      "\n",
      "    accuracy                           0.63       624\n",
      "   macro avg       0.82      0.51      0.41       624\n",
      "weighted avg       0.77      0.63      0.50       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(data_train, label_train)\n",
    "\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102 132]\n",
      " [ 86 304]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.44      0.48       234\n",
      "           1       0.70      0.78      0.74       390\n",
      "\n",
      "    accuracy                           0.65       624\n",
      "   macro avg       0.62      0.61      0.61       624\n",
      "weighted avg       0.64      0.65      0.64       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='sigmoid')\n",
    "svclassifier.fit(data_train, label_train)\n",
    "\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:svm] *",
   "language": "python",
   "name": "conda-env-svm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
