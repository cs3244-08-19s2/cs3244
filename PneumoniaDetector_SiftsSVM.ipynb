{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import os\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%aimport utils\n",
    "%aimport imc\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdiv = \"/\"\n",
    "### LABELS ###\n",
    "\n",
    "normal_label = 0\n",
    "pneumonia_label = 1\n",
    "labels = sorted([normal_label, pneumonia_label])\n",
    "\n",
    "### IMAGE SETTINGS ###\n",
    "\n",
    "dimension = (64,0)\n",
    "resize_dim = dimension[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_train_normal = \".{}chest_xray{}train{}NORMAL{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "path_train_pneumonia = \".{}chest_xray{}train{}PNEUMONIA{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "\n",
    "path_test_normal = \".{}chest_xray{}test{}NORMAL{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "path_test_pneumonia = \".{}chest_xray{}test{}PNEUMONIA{}\".format(pdiv, pdiv, pdiv, pdiv)\n",
    "\n",
    "# number of images to be loaded from each directory\n",
    "train_images_limit = 200\n",
    "test_images_limit = 200\n",
    "load_all_images = True\n",
    "\n",
    "paths_train_normal = utils.extract_image_paths(path_train_normal)\n",
    "paths_train_pneumonia = utils.extract_image_paths(path_train_pneumonia)\n",
    "paths_test_normal = utils.extract_image_paths(path_test_normal)\n",
    "paths_test_pneumonia = utils.extract_image_paths(path_test_pneumonia)\n",
    "\n",
    "if not load_all_images:\n",
    "    paths_train_normal = paths_train_normal[1:train_images_limit]\n",
    "    paths_train_pneumonia = paths_train_pneumonia[1:train_images_limit]\n",
    "    paths_test_normal = paths_test_normal[1:test_images_limit]\n",
    "    paths_test_pneumonia = paths_train_pneumonia[1:test_images_limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Sift Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 40-word vocabulary from training dataset, saved to vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# from imc import build_vocabulary_from_dirs\n",
    "import os.path as osp\n",
    "import pickle\n",
    "\n",
    "vocab_filename = 'vocab.pkl'\n",
    "vocab_size = 40\n",
    "vocab = imc.build_vocabulary_from_dirs(paths_train_normal, paths_train_pneumonia, vocab_size)\n",
    "with open(vocab_filename, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    \n",
    "print(f\"Built {vocab_size}-word vocabulary from training dataset, saved to {vocab_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Image Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image histograms generated for training and test images\n"
     ]
    }
   ],
   "source": [
    "from imc import bags_of_sifts\n",
    "\n",
    "data_train_normal = bags_of_sifts(paths_train_normal, vocab_filename)\n",
    "data_train_pneumonia = bags_of_sifts(paths_train_pneumonia, vocab_filename)\n",
    "\n",
    "data_test_normal = bags_of_sifts(paths_test_normal, vocab_filename)\n",
    "data_test_pneumonia = bags_of_sifts(paths_test_pneumonia, vocab_filename)\n",
    "\n",
    "print(\"Image histograms generated for training and test images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data concatenated\n",
      "Train data dimensions: (5216, 200)\n",
      "Test data dimensions: (467, 200)\n"
     ]
    }
   ],
   "source": [
    "# Number of training and test images for normal and pneumonia\n",
    "\n",
    "label_train_normal = [0] * len(paths_train_normal)\n",
    "label_train_pneumonia = [1] * len(paths_train_pneumonia)\n",
    "label_test_normal = [0] * len(paths_test_normal)\n",
    "label_test_pneumonia = [1] * len(paths_test_pneumonia)\n",
    "\n",
    "# Combine training images and labels\n",
    "data_train = np.concatenate((data_train_normal, data_train_pneumonia), axis=0)\n",
    "label_train = np.asarray(label_train_normal + label_train_pneumonia)\n",
    "\n",
    "# Combine training images and labels\n",
    "data_test = np.concatenate((data_test_normal, data_test_pneumonia[1:234]), axis=0)\n",
    "label_test = np.asarray(label_test_normal + label_test_pneumonia[1:234])\n",
    "\n",
    "print (\"Data concatenated\")\n",
    "print (f\"Train data dimensions: {data_train.shape}\")\n",
    "print (f\"Test data dimensions: {data_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversample Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7750, 200)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, KMeansSMOTE, BorderlineSMOTE\n",
    "# import imblearn\n",
    "oversample = BorderlineSMOTE(random_state=100)\n",
    "data_train, label_train = oversample.fit_resample(data_train.reshape(data_train.shape[0], -1), label_train)\n",
    "\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets shuffled\n",
      "(7750, 200)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "rand_order = np.random.permutation(data_train.shape[0])\n",
    "\n",
    "data_train = data_train[rand_order]\n",
    "label_train = label_train[rand_order]\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "rand_order = np.random.permutation(data_test.shape[0])\n",
    "data_test = data_test[rand_order]\n",
    "label_test = label_test[rand_order]\n",
    "\n",
    "print(\"Datasets shuffled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Scale Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 17 217]\n",
      " [  1 232]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.07      0.13       234\n",
      "           1       0.52      1.00      0.68       233\n",
      "\n",
      "    accuracy                           0.53       467\n",
      "   macro avg       0.73      0.53      0.41       467\n",
      "weighted avg       0.73      0.53      0.41       467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(gamma='scale')\n",
    "svclassifier.fit(data_train, label_train)\n",
    "      \n",
    "# Make prediction\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 44 190]\n",
      " [  3 230]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.19      0.31       234\n",
      "           1       0.55      0.99      0.70       233\n",
      "\n",
      "    accuracy                           0.59       467\n",
      "   macro avg       0.74      0.59      0.51       467\n",
      "weighted avg       0.74      0.59      0.51       467\n",
      "\n",
      "### CONFUSION MATRIX ###\n",
      "[[ 44 190]\n",
      " [  3 230]]\n",
      "\n",
      " ### TEST METRICS ###\n",
      "Accuracy: 0.6346153846153846%\n",
      "Precision: 63.10679611650486%\n",
      "Recall: 100.0%\n",
      "F1-score: 77.38095238095238\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='poly', degree=8)\n",
    "svclassifier.fit(data_train, label_train)\n",
    "\n",
    "# Make prediction\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))\n",
    "\n",
    "\n",
    "print('### CONFUSION MATRIX ###')\n",
    "print(cm)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "acc = (tp + tn) / sum([tn, fp, fn, tp])\n",
    "\n",
    "### Calculate Recall and Precision ###\n",
    "print('\\n ### TEST METRICS ###')\n",
    "precision = tp/(tp+fp)*100\n",
    "recall = tp/(tp+fn)*100\n",
    "print('Accuracy: {}%'.format(acc))\n",
    "print('Precision: {}%'.format(precision))\n",
    "print('Recall: {}%'.format(recall))\n",
    "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 17 217]\n",
      " [  1 232]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.07      0.13       234\n",
      "           1       0.52      1.00      0.68       233\n",
      "\n",
      "    accuracy                           0.53       467\n",
      "   macro avg       0.73      0.53      0.41       467\n",
      "weighted avg       0.73      0.53      0.41       467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')\n",
    "svclassifier.fit(data_train, label_train)\n",
    "\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "print(classification_report(label_test, label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.52      0.61       234\n",
      "           1       0.63      0.83      0.72       233\n",
      "\n",
      "    accuracy                           0.67       467\n",
      "   macro avg       0.69      0.67      0.66       467\n",
      "weighted avg       0.69      0.67      0.66       467\n",
      "\n",
      "### CONFUSION MATRIX ###\n",
      "[[121 113]\n",
      " [ 40 193]]\n",
      "\n",
      " ### TEST METRICS ###\n",
      "Accuracy: 0.6875%\n",
      "Precision: 72.20956719817767%\n",
      "Recall: 81.28205128205128%\n",
      "F1-score: 76.47768395657418\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='sigmoid')\n",
    "svclassifier.fit(data_train, label_train)\n",
    "\n",
    "label_pred = svclassifier.predict(data_test)\n",
    "cm = confusion_matrix(label_test, label_pred)\n",
    "print(classification_report(label_test, label_pred))\n",
    "\n",
    "\n",
    "print('### CONFUSION MATRIX ###')\n",
    "acc = (tp + tn) / sum([tn, fp, fn, tp])\n",
    "\n",
    "### Calculate Recall and Precision ###\n",
    "print('\\n ### TEST METRICS ###')\n",
    "precision = tp/(tp+fp)*100\n",
    "recall = tp/(tp+fn)*100\n",
    "print('Accuracy: {}%'.format(acc))\n",
    "print('Precision: {}%'.format(precision))\n",
    "print('Recall: {}%'.format(recall))\n",
    "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:svm]",
   "language": "python",
   "name": "conda-env-svm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
